# Kafkaæ¶ˆæ¯é˜Ÿåˆ—è§„èŒƒ

## ğŸ¯ è§„èŒƒç›®æ ‡
- å»ºç«‹Kafkaæ¶ˆæ¯ç³»ç»Ÿçš„ç»Ÿä¸€ä½¿ç”¨æ ‡å‡†
- ç¡®ä¿æ¶ˆæ¯çš„å¯é ä¼ è¾“å’Œé«˜æ€§èƒ½å¤„ç†
- æä¾›æ¶ˆæ¯ç‰ˆæœ¬ç®¡ç†å’Œå‘åå…¼å®¹ç­–ç•¥
- å»ºç«‹å®Œå–„çš„ç›‘æ§å’Œè¿ç»´ä½“ç³»

## ğŸ“‹ ä¸»é¢˜è®¾è®¡è§„èŒƒ

### ä¸»é¢˜å‘½åè§„èŒƒ

#### MUST - å¿…é¡»éµå®ˆ
```text
âœ… æ ‡å‡†å‘½åæ ¼å¼ï¼š{ç¯å¢ƒ}.{æœåŠ¡}.{ä¸šåŠ¡åŸŸ}.{äº‹ä»¶ç±»å‹}.{ç‰ˆæœ¬}

æ­£ç¡®ç¤ºä¾‹ï¼š
dev.user-service.account.user-created.v1
prod.order-service.payment.payment-completed.v1
test.notification-service.email.email-sent.v2

âŒ é”™è¯¯ç¤ºä¾‹ï¼š
userCreated                    # ç¼ºå°‘ç¯å¢ƒå’ŒæœåŠ¡ä¿¡æ¯
user.created                   # ç¼ºå°‘ç‰ˆæœ¬ä¿¡æ¯
UserCreatedEvent               # ä½¿ç”¨é©¼å³°å‘½å
user_created_event             # ä½¿ç”¨ä¸‹åˆ’çº¿
```

### ä¸»é¢˜é…ç½®è§„èŒƒ
```java
// [AI-BLOCK-START] - ç”Ÿæˆå·¥å…·: Claude 3.5 Sonnet
/**
 * Kafkaä¸»é¢˜é…ç½®å¸¸é‡
 */
public class KafkaTopicConfig {
    
    // ç¯å¢ƒå‰ç¼€
    public static final String ENV_DEV = "dev";
    public static final String ENV_TEST = "test";
    public static final String ENV_PROD = "prod";
    
    // æœåŠ¡åç§°
    public static final String USER_SERVICE = "user-service";
    public static final String ORDER_SERVICE = "order-service";
    public static final String PAYMENT_SERVICE = "payment-service";
    public static final String NOTIFICATION_SERVICE = "notification-service";
    
    // ä¸šåŠ¡åŸŸ
    public static final String DOMAIN_ACCOUNT = "account";
    public static final String DOMAIN_ORDER = "order";
    public static final String DOMAIN_PAYMENT = "payment";
    public static final String DOMAIN_NOTIFICATION = "notification";
    
    // äº‹ä»¶ç±»å‹
    public static final String EVENT_CREATED = "created";
    public static final String EVENT_UPDATED = "updated";
    public static final String EVENT_DELETED = "deleted";
    public static final String EVENT_COMPLETED = "completed";
    public static final String EVENT_FAILED = "failed";
    
    /**
     * æ„å»ºä¸»é¢˜åç§°
     */
    public static String buildTopicName(String env, String service, String domain, 
                                       String eventType, String version) {
        return String.join(".", env, service, domain, eventType, version);
    }
}

/**
 * ä¸»é¢˜ç®¡ç†æœåŠ¡
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class KafkaTopicManager {
    
    private final KafkaAdmin kafkaAdmin;
    
    @Value("${spring.profiles.active:dev}")
    private String environment;
    
    /**
     * åˆ›å»ºä¸»é¢˜
     */
    public void createTopic(String service, String domain, String eventType, 
                           String version, int partitions, short replicationFactor) {
        
        String topicName = KafkaTopicConfig.buildTopicName(
            environment, service, domain, eventType, version);
        
        NewTopic topic = TopicBuilder.name(topicName)
            .partitions(partitions)
            .replicas(replicationFactor)
            .config(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_DELETE)
            .config(TopicConfig.RETENTION_MS_CONFIG, "604800000") // 7å¤©
            .config(TopicConfig.COMPRESSION_TYPE_CONFIG, "lz4")
            .build();
        
        try {
            kafkaAdmin.createOrModifyTopics(topic);
            log.info("Kafkaä¸»é¢˜åˆ›å»ºæˆåŠŸ: {}", topicName);
        } catch (Exception e) {
            log.error("Kafkaä¸»é¢˜åˆ›å»ºå¤±è´¥: {}", topicName, e);
        }
    }
    
    /**
     * è·å–ä¸»é¢˜ä¿¡æ¯
     */
    public void describeTopics(String... topicNames) {
        try {
            DescribeTopicsResult result = kafkaAdmin.describeTopics(Arrays.asList(topicNames));
            result.all().get().forEach((name, description) -> {
                log.info("ä¸»é¢˜ä¿¡æ¯: name={}, partitions={}", 
                    name, description.partitions().size());
            });
        } catch (Exception e) {
            log.error("è·å–ä¸»é¢˜ä¿¡æ¯å¤±è´¥", e);
        }
    }
}
// [AI-BLOCK-END]
```

## ğŸ“‹ æ¶ˆæ¯æ ¼å¼è§„èŒƒ

### æ ‡å‡†æ¶ˆæ¯ç»“æ„
```java
// [AI-BLOCK-START] - ç”Ÿæˆå·¥å…·: Claude 3.5 Sonnet
/**
 * æ ‡å‡†æ¶ˆæ¯åŸºç±»
 */
@Data
@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "messageType")
@JsonSubTypes({
    @JsonSubTypes.Type(value = UserCreatedEvent.class, name = "UserCreatedEvent"),
    @JsonSubTypes.Type(value = OrderCompletedEvent.class, name = "OrderCompletedEvent"),
    @JsonSubTypes.Type(value = PaymentProcessedEvent.class, name = "PaymentProcessedEvent")
})
public abstract class BaseMessage {
    
    /**
     * æ¶ˆæ¯IDï¼Œå…¨å±€å”¯ä¸€
     */
    @JsonProperty("messageId")
    private String messageId = UUID.randomUUID().toString();
    
    /**
     * æ¶ˆæ¯ç±»å‹
     */
    @JsonProperty("messageType")
    private String messageType;
    
    /**
     * æ¶ˆæ¯ç‰ˆæœ¬
     */
    @JsonProperty("version")
    private String version = "1.0";
    
    /**
     * äº‹ä»¶æ—¶é—´æˆ³
     */
    @JsonProperty("timestamp")
    private Long timestamp = System.currentTimeMillis();
    
    /**
     * æ¥æºæœåŠ¡
     */
    @JsonProperty("source")
    private String source;
    
    /**
     * å…³è”IDï¼ˆç”¨äºæ¶ˆæ¯è¿½è¸ªï¼‰
     */
    @JsonProperty("correlationId")
    private String correlationId;
    
    /**
     * é‡è¯•æ¬¡æ•°
     */
    @JsonProperty("retryCount")
    private Integer retryCount = 0;
    
    /**
     * æ¶ˆæ¯å¤´éƒ¨ä¿¡æ¯
     */
    @JsonProperty("headers")
    private Map<String, String> headers = new HashMap<>();
    
    public BaseMessage() {
        this.messageType = this.getClass().getSimpleName();
    }
}

/**
 * ç”¨æˆ·åˆ›å»ºäº‹ä»¶
 */
@Data
@EqualsAndHashCode(callSuper = true)
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class UserCreatedEvent extends BaseMessage {
    
    @JsonProperty("userId")
    private Long userId;
    
    @JsonProperty("username")
    private String username;
    
    @JsonProperty("email")
    private String email;
    
    @JsonProperty("createdBy")
    private String createdBy;
    
    @JsonProperty("userProfile")
    private UserProfile userProfile;
    
    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class UserProfile {
        private String firstName;
        private String lastName;
        private String phone;
        private String address;
    }
    
    public UserCreatedEvent(User user) {
        super();
        this.userId = user.getId();
        this.username = user.getUsername();
        this.email = user.getEmail();
        this.createdBy = user.getCreatedBy();
        this.setSource("user-service");
        
        if (user.getProfile() != null) {
            this.userProfile = UserProfile.builder()
                .firstName(user.getProfile().getFirstName())
                .lastName(user.getProfile().getLastName())
                .phone(user.getProfile().getPhone())
                .address(user.getProfile().getAddress())
                .build();
        }
    }
}

/**
 * è®¢å•å®Œæˆäº‹ä»¶
 */
@Data
@EqualsAndHashCode(callSuper = true)
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class OrderCompletedEvent extends BaseMessage {
    
    @JsonProperty("orderId")
    private Long orderId;
    
    @JsonProperty("orderNo")
    private String orderNo;
    
    @JsonProperty("userId")
    private Long userId;
    
    @JsonProperty("totalAmount")
    private BigDecimal totalAmount;
    
    @JsonProperty("orderItems")
    private List<OrderItem> orderItems;
    
    @JsonProperty("completedTime")
    private LocalDateTime completedTime;
    
    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class OrderItem {
        private Long productId;
        private String productName;
        private Integer quantity;
        private BigDecimal price;
    }
    
    public OrderCompletedEvent(Order order) {
        super();
        this.orderId = order.getId();
        this.orderNo = order.getOrderNo();
        this.userId = order.getUserId();
        this.totalAmount = order.getTotalAmount();
        this.completedTime = order.getCompletedTime();
        this.setSource("order-service");
        
        if (order.getItems() != null) {
            this.orderItems = order.getItems().stream()
                .map(item -> OrderItem.builder()
                    .productId(item.getProductId())
                    .productName(item.getProductName())
                    .quantity(item.getQuantity())
                    .price(item.getPrice())
                    .build())
                .collect(Collectors.toList());
        }
    }
}
// [AI-BLOCK-END]
```

## ğŸ“‹ ç”Ÿäº§è€…è§„èŒƒ

### ç”Ÿäº§è€…é…ç½®
```yaml
# âœ… Kafkaç”Ÿäº§è€…é…ç½®
spring:
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      # åºåˆ—åŒ–é…ç½®
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      
      # æ€§èƒ½é…ç½®
      batch-size: 16384        # æ‰¹é‡å‘é€å¤§å° 16KB
      linger-ms: 10           # æ‰¹é‡å‘é€å»¶è¿Ÿæ—¶é—´
      buffer-memory: 33554432 # ç¼“å†²åŒºå¤§å° 32MB
      
      # å¯é æ€§é…ç½®
      acks: all               # ç­‰å¾…æ‰€æœ‰å‰¯æœ¬ç¡®è®¤
      retries: 3              # é‡è¯•æ¬¡æ•°
      retry-backoff-ms: 1000  # é‡è¯•é—´éš”
      
      # å¹‚ç­‰æ€§é…ç½®
      enable-idempotence: true
      max-in-flight-requests-per-connection: 1
      
      # å‹ç¼©é…ç½®
      compression-type: lz4
      
      # è‡ªå®šä¹‰é…ç½®
      properties:
        max.request.size: 1048576  # 1MB
        request.timeout.ms: 30000  # 30ç§’
```

### ç”Ÿäº§è€…å®ç°
```java
// [AI-BLOCK-START] - ç”Ÿæˆå·¥å…·: Claude 3.5 Sonnet
/**
 * Kafkaæ¶ˆæ¯ç”Ÿäº§è€…æœåŠ¡
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class KafkaMessageProducer {
    
    private final KafkaTemplate<String, Object> kafkaTemplate;
    
    @Value("${spring.profiles.active:dev}")
    private String environment;
    
    /**
     * å‘é€ç”¨æˆ·åˆ›å»ºäº‹ä»¶
     */
    public void sendUserCreatedEvent(User user) {
        UserCreatedEvent event = new UserCreatedEvent(user);
        event.setCorrelationId(generateCorrelationId());
        
        String topicName = KafkaTopicConfig.buildTopicName(
            environment, KafkaTopicConfig.USER_SERVICE, 
            KafkaTopicConfig.DOMAIN_ACCOUNT, KafkaTopicConfig.EVENT_CREATED, "v1");
        
        sendMessage(topicName, user.getId().toString(), event);
    }
    
    /**
     * å‘é€è®¢å•å®Œæˆäº‹ä»¶
     */
    public void sendOrderCompletedEvent(Order order) {
        OrderCompletedEvent event = new OrderCompletedEvent(order);
        event.setCorrelationId(generateCorrelationId());
        
        String topicName = KafkaTopicConfig.buildTopicName(
            environment, KafkaTopicConfig.ORDER_SERVICE,
            KafkaTopicConfig.DOMAIN_ORDER, KafkaTopicConfig.EVENT_COMPLETED, "v1");
        
        sendMessage(topicName, order.getOrderNo(), event);
    }
    
    /**
     * é€šç”¨æ¶ˆæ¯å‘é€æ–¹æ³•
     */
    public void sendMessage(String topicName, String key, Object message) {
        try {
            // è®¾ç½®æ¶ˆæ¯å¤´
            ProducerRecord<String, Object> record = new ProducerRecord<>(topicName, key, message);
            record.headers().add("messageType", message.getClass().getSimpleName().getBytes());
            record.headers().add("timestamp", String.valueOf(System.currentTimeMillis()).getBytes());
            record.headers().add("source", environment.getBytes());
            
            // å¼‚æ­¥å‘é€æ¶ˆæ¯
            ListenableFuture<SendResult<String, Object>> future = kafkaTemplate.send(record);
            
            // æ·»åŠ å›è°ƒå¤„ç†
            future.addCallback(new ListenableFutureCallback<SendResult<String, Object>>() {
                @Override
                public void onSuccess(SendResult<String, Object> result) {
                    RecordMetadata metadata = result.getRecordMetadata();
                    log.info("æ¶ˆæ¯å‘é€æˆåŠŸ: topic={}, partition={}, offset={}, key={}", 
                        metadata.topic(), metadata.partition(), metadata.offset(), key);
                }
                
                @Override
                public void onFailure(Throwable ex) {
                    log.error("æ¶ˆæ¯å‘é€å¤±è´¥: topic={}, key={}, message={}", 
                        topicName, key, message, ex);
                    
                    // å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—æˆ–é‡è¯•é˜Ÿåˆ—
                    handleSendFailure(topicName, key, message, ex);
                }
            });
            
        } catch (Exception e) {
            log.error("æ¶ˆæ¯å‘é€å¼‚å¸¸: topic={}, key={}", topicName, key, e);
            throw new MessageSendException("æ¶ˆæ¯å‘é€å¤±è´¥", e);
        }
    }
    
    /**
     * æ‰¹é‡å‘é€æ¶ˆæ¯
     */
    public void sendBatchMessages(String topicName, Map<String, Object> messages) {
        List<ProducerRecord<String, Object>> records = messages.entrySet().stream()
            .map(entry -> {
                ProducerRecord<String, Object> record = new ProducerRecord<>(
                    topicName, entry.getKey(), entry.getValue());
                record.headers().add("batchId", generateBatchId().getBytes());
                return record;
            })
            .collect(Collectors.toList());
        
        // ä½¿ç”¨äº‹åŠ¡å‘é€æ‰¹é‡æ¶ˆæ¯
        kafkaTemplate.executeInTransaction(operations -> {
            records.forEach(record -> operations.send(record));
            return null;
        });
        
        log.info("æ‰¹é‡æ¶ˆæ¯å‘é€å®Œæˆ: topic={}, count={}", topicName, messages.size());
    }
    
    /**
     * å‘é€å»¶è¿Ÿæ¶ˆæ¯ï¼ˆé€šè¿‡æ—¶é—´æˆ³ï¼‰
     */
    public void sendDelayedMessage(String topicName, String key, Object message, long delayMs) {
        Timer timer = new Timer();
        timer.schedule(new TimerTask() {
            @Override
            public void run() {
                sendMessage(topicName, key, message);
            }
        }, delayMs);
        
        log.info("å»¶è¿Ÿæ¶ˆæ¯å·²å®‰æ’: topic={}, key={}, delay={}ms", topicName, key, delayMs);
    }
    
    private void handleSendFailure(String topicName, String key, Object message, Throwable ex) {
        // æ„å»ºå¤±è´¥æ¶ˆæ¯è®°å½•
        FailedMessage failedMessage = FailedMessage.builder()
            .originalTopic(topicName)
            .key(key)
            .message(message)
            .errorMessage(ex.getMessage())
            .failureTime(LocalDateTime.now())
            .retryCount(0)
            .build();
        
        // å‘é€åˆ°é‡è¯•é˜Ÿåˆ—
        String retryTopic = topicName + ".retry";
        try {
            kafkaTemplate.send(retryTopic, key, failedMessage);
            log.info("å¤±è´¥æ¶ˆæ¯å·²å‘é€åˆ°é‡è¯•é˜Ÿåˆ—: retryTopic={}, key={}", retryTopic, key);
        } catch (Exception retryEx) {
            log.error("å‘é€åˆ°é‡è¯•é˜Ÿåˆ—å¤±è´¥: retryTopic={}, key={}", retryTopic, key, retryEx);
        }
    }
    
    private String generateCorrelationId() {
        return "corr-" + UUID.randomUUID().toString();
    }
    
    private String generateBatchId() {
        return "batch-" + System.currentTimeMillis() + "-" + ThreadLocalRandom.current().nextInt(1000);
    }
}

/**
 * å¤±è´¥æ¶ˆæ¯è®°å½•
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class FailedMessage {
    private String originalTopic;
    private String key;
    private Object message;
    private String errorMessage;
    private LocalDateTime failureTime;
    private Integer retryCount;
}
// [AI-BLOCK-END]
```

## ğŸ“‹ æ¶ˆè´¹è€…è§„èŒƒ

### æ¶ˆè´¹è€…é…ç½®
```yaml
# âœ… Kafkaæ¶ˆè´¹è€…é…ç½®
spring:
  kafka:
    consumer:
      # åŸºç¡€é…ç½®
      group-id: ${spring.application.name}-consumer-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      
      # æ¶ˆè´¹é…ç½®
      auto-offset-reset: earliest
      enable-auto-commit: false  # æ‰‹åŠ¨æäº¤åç§»é‡
      fetch-min-size: 1
      fetch-max-wait: 500
      max-poll-records: 500
      
      # JSONååºåˆ—åŒ–é…ç½®
      properties:
        spring.json.trusted.packages: "cn.org.bjca.footstone.event"
        spring.json.type.mapping: |
          UserCreatedEvent:cn.org.bjca.footstone.event.UserCreatedEvent,
          OrderCompletedEvent:cn.org.bjca.footstone.event.OrderCompletedEvent
        
    # ç›‘å¬å™¨é…ç½®
    listener:
      ack-mode: manual_immediate  # æ‰‹åŠ¨ç¡®è®¤æ¨¡å¼
      concurrency: 3              # å¹¶å‘æ¶ˆè´¹è€…æ•°é‡
      poll-timeout: 3000          # è½®è¯¢è¶…æ—¶æ—¶é—´
      type: batch                 # æ‰¹é‡æ¶ˆè´¹æ¨¡å¼
```

### æ¶ˆè´¹è€…å®ç°
```java
// [AI-BLOCK-START] - ç”Ÿæˆå·¥å…·: Claude 3.5 Sonnet
/**
 * ç”¨æˆ·äº‹ä»¶æ¶ˆè´¹è€…
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class UserEventConsumer {
    
    private final UserService userService;
    private final NotificationService notificationService;
    private final MeterRegistry meterRegistry;
    
    private final Counter processedCounter = Counter.builder("kafka.message.processed")
        .description("å¤„ç†çš„æ¶ˆæ¯æ•°é‡")
        .register(meterRegistry);
    
    private final Timer processingTimer = Timer.builder("kafka.message.processing.time")
        .description("æ¶ˆæ¯å¤„ç†æ—¶é—´")
        .register(meterRegistry);
    
    /**
     * æ¶ˆè´¹ç”¨æˆ·åˆ›å»ºäº‹ä»¶
     */
    @KafkaListener(
        topics = "${app.kafka.topics.user-created:dev.user-service.account.user-created.v1}",
        groupId = "${spring.application.name}-user-consumer",
        containerFactory = "kafkaListenerContainerFactory"
    )
    public void handleUserCreatedEvent(
            @Payload UserCreatedEvent event,
            @Header Map<String, Object> headers,
            Acknowledgment acknowledgment,
            ConsumerRecord<String, UserCreatedEvent> record) {
        
        Timer.Sample sample = Timer.start(meterRegistry);
        
        try {
            log.info("æ”¶åˆ°ç”¨æˆ·åˆ›å»ºäº‹ä»¶: userId={}, messageId={}, offset={}", 
                event.getUserId(), event.getMessageId(), record.offset());
            
            // å¹‚ç­‰æ€§æ£€æŸ¥
            if (isMessageProcessed(event.getMessageId())) {
                log.info("æ¶ˆæ¯å·²å¤„ç†ï¼Œè·³è¿‡: messageId={}", event.getMessageId());
                acknowledgment.acknowledge();
                return;
            }
            
            // å¤„ç†ä¸šåŠ¡é€»è¾‘
            processUserCreatedEvent(event);
            
            // æ ‡è®°æ¶ˆæ¯å·²å¤„ç†
            markMessageProcessed(event.getMessageId());
            
            // æ‰‹åŠ¨ç¡®è®¤æ¶ˆæ¯
            acknowledgment.acknowledge();
            
            // è®°å½•æŒ‡æ ‡
            processedCounter.increment(Tags.of("event", "user-created", "status", "success"));
            
            log.info("ç”¨æˆ·åˆ›å»ºäº‹ä»¶å¤„ç†å®Œæˆ: userId={}, messageId={}", 
                event.getUserId(), event.getMessageId());
            
        } catch (Exception e) {
            log.error("ç”¨æˆ·åˆ›å»ºäº‹ä»¶å¤„ç†å¤±è´¥: userId={}, messageId={}", 
                event.getUserId(), event.getMessageId(), e);
            
            // è®°å½•å¤±è´¥æŒ‡æ ‡
            processedCounter.increment(Tags.of("event", "user-created", "status", "error"));
            
            // å¤„ç†å¤±è´¥é€»è¾‘
            handleProcessingFailure(event, e, acknowledgment);
            
        } finally {
            sample.stop(processingTimer);
        }
    }
    
    /**
     * æ‰¹é‡æ¶ˆè´¹è®¢å•äº‹ä»¶
     */
    @KafkaListener(
        topics = "${app.kafka.topics.order-completed:dev.order-service.order.order-completed.v1}",
        groupId = "${spring.application.name}-order-consumer",
        containerFactory = "batchKafkaListenerContainerFactory"
    )
    public void handleOrderCompletedEvents(
            List<ConsumerRecord<String, OrderCompletedEvent>> records,
            Acknowledgment acknowledgment) {
        
        try {
            log.info("æ”¶åˆ°æ‰¹é‡è®¢å•å®Œæˆäº‹ä»¶: count={}", records.size());
            
            List<OrderCompletedEvent> events = records.stream()
                .map(ConsumerRecord::value)
                .collect(Collectors.toList());
            
            // æ‰¹é‡å¤„ç†äº‹ä»¶
            processOrderCompletedEventsBatch(events);
            
            // ç¡®è®¤æ‰€æœ‰æ¶ˆæ¯
            acknowledgment.acknowledge();
            
            processedCounter.increment(Tags.of("event", "order-completed", "status", "success"), 
                records.size());
            
            log.info("æ‰¹é‡è®¢å•å®Œæˆäº‹ä»¶å¤„ç†å®Œæˆ: count={}", records.size());
            
        } catch (Exception e) {
            log.error("æ‰¹é‡è®¢å•å®Œæˆäº‹ä»¶å¤„ç†å¤±è´¥: count={}", records.size(), e);
            
            // é€ä¸ªå¤„ç†å¤±è´¥çš„æ¶ˆæ¯
            handleBatchProcessingFailure(records, e, acknowledgment);
        }
    }
    
    /**
     * é”™è¯¯ä¸»é¢˜æ¶ˆè´¹è€…
     */
    @KafkaListener(
        topics = "${app.kafka.topics.error:error-topic}",
        groupId = "${spring.application.name}-error-consumer"
    )
    public void handleErrorMessages(
            @Payload Object errorMessage,
            @Header Map<String, Object> headers,
            Acknowledgment acknowledgment) {
        
        try {
            log.warn("æ”¶åˆ°é”™è¯¯æ¶ˆæ¯: headers={}, message={}", headers, errorMessage);
            
            // è®°å½•é”™è¯¯æ¶ˆæ¯åˆ°æ•°æ®åº“æˆ–ç›‘æ§ç³»ç»Ÿ
            recordErrorMessage(errorMessage, headers);
            
            acknowledgment.acknowledge();
            
        } catch (Exception e) {
            log.error("é”™è¯¯æ¶ˆæ¯å¤„ç†å¤±è´¥", e);
        }
    }
    
    private void processUserCreatedEvent(UserCreatedEvent event) {
        // å‘é€æ¬¢è¿é‚®ä»¶
        notificationService.sendWelcomeEmail(event.getEmail(), event.getUsername());
        
        // åˆ›å»ºç”¨æˆ·ç»Ÿè®¡è®°å½•
        userService.createUserStats(event.getUserId());
        
        // åˆå§‹åŒ–ç”¨æˆ·åå¥½è®¾ç½®
        userService.initializeUserPreferences(event.getUserId());
    }
    
    private void processOrderCompletedEventsBatch(List<OrderCompletedEvent> events) {
        // æ‰¹é‡æ›´æ–°ç”¨æˆ·ç§¯åˆ†
        Map<Long, BigDecimal> userPoints = events.stream()
            .collect(Collectors.groupingBy(
                OrderCompletedEvent::getUserId,
                Collectors.reducing(BigDecimal.ZERO, 
                    OrderCompletedEvent::getTotalAmount, BigDecimal::add)));
        
        userService.batchUpdateUserPoints(userPoints);
        
        // æ‰¹é‡å‘é€è®¢å•å®Œæˆé€šçŸ¥
        List<String> emails = events.stream()
            .map(event -> userService.getUserEmail(event.getUserId()))
            .collect(Collectors.toList());
        
        notificationService.batchSendOrderCompletedNotifications(emails, events);
    }
    
    private boolean isMessageProcessed(String messageId) {
        // æ£€æŸ¥Redisæˆ–æ•°æ®åº“ä¸­æ˜¯å¦å·²å¤„ç†è¯¥æ¶ˆæ¯
        return userService.isMessageProcessed(messageId);
    }
    
    private void markMessageProcessed(String messageId) {
        // åœ¨Redisæˆ–æ•°æ®åº“ä¸­æ ‡è®°æ¶ˆæ¯å·²å¤„ç†
        userService.markMessageProcessed(messageId);
    }
    
    private void handleProcessingFailure(UserCreatedEvent event, Exception e, 
                                       Acknowledgment acknowledgment) {
        
        // å¢åŠ é‡è¯•æ¬¡æ•°
        event.setRetryCount(event.getRetryCount() + 1);
        
        if (event.getRetryCount() >= 3) {
            // è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
            sendToDeadLetterQueue(event, e);
            acknowledgment.acknowledge(); // ç¡®è®¤æ¶ˆæ¯ï¼Œé¿å…æ— é™é‡è¯•
        } else {
            // å‘é€åˆ°é‡è¯•é˜Ÿåˆ—
            sendToRetryQueue(event);
            acknowledgment.acknowledge();
        }
    }
    
    private void handleBatchProcessingFailure(List<ConsumerRecord<String, OrderCompletedEvent>> records, 
                                            Exception e, Acknowledgment acknowledgment) {
        // æ‰¹é‡å¤„ç†å¤±è´¥æ—¶ï¼Œé€ä¸ªé‡è¯•
        for (ConsumerRecord<String, OrderCompletedEvent> record : records) {
            try {
                processOrderCompletedEventsBatch(Collections.singletonList(record.value()));
            } catch (Exception individualException) {
                log.error("å•ä¸ªè®¢å•äº‹ä»¶å¤„ç†å¤±è´¥: orderId={}", 
                    record.value().getOrderId(), individualException);
                sendToDeadLetterQueue(record.value(), individualException);
            }
        }
        
        acknowledgment.acknowledge();
    }
    
    private void sendToRetryQueue(Object event) {
        // å‘é€åˆ°é‡è¯•ä¸»é¢˜
        String retryTopic = "retry-topic";
        // kafkaTemplate.send(retryTopic, event);
        log.info("æ¶ˆæ¯å·²å‘é€åˆ°é‡è¯•é˜Ÿåˆ—: messageId={}", 
            event instanceof BaseMessage ? ((BaseMessage) event).getMessageId() : "unknown");
    }
    
    private void sendToDeadLetterQueue(Object event, Exception e) {
        // å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
        String dlqTopic = "dead-letter-topic";
        // kafkaTemplate.send(dlqTopic, event);
        log.error("æ¶ˆæ¯å·²å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—: messageId={}, error={}", 
            event instanceof BaseMessage ? ((BaseMessage) event).getMessageId() : "unknown", 
            e.getMessage());
    }
    
    private void recordErrorMessage(Object errorMessage, Map<String, Object> headers) {
        // è®°å½•é”™è¯¯æ¶ˆæ¯åˆ°ç›‘æ§ç³»ç»Ÿ
        log.error("è®°å½•é”™è¯¯æ¶ˆæ¯: headers={}, message={}", headers, errorMessage);
    }
}
// [AI-BLOCK-END]
```

## ğŸ“‹ äº‹åŠ¡å’Œå¯é æ€§

### äº‹åŠ¡ç”Ÿäº§è€…é…ç½®
```java
// [AI-BLOCK-START] - ç”Ÿæˆå·¥å…·: Claude 3.5 Sonnet
/**
 * Kafkaäº‹åŠ¡é…ç½®
 */
@Configuration
@EnableKafka
@EnableTransactionManagement
public class KafkaTransactionConfig {
    
    @Bean
    @Primary
    public ProducerFactory<String, Object> producerFactory() {
        Map<String, Object> props = new HashMap<>();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
        
        // äº‹åŠ¡é…ç½®
        props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "tx-");
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        props.put(ProducerConfig.ACKS_CONFIG, "all");
        props.put(ProducerConfig.RETRIES_CONFIG, 3);
        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1);
        
        return new DefaultKafkaProducerFactory<>(props);
    }
    
    @Bean
    public KafkaTemplate<String, Object> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }
    
    @Bean
    public PlatformTransactionManager kafkaTransactionManager() {
        KafkaTransactionManager manager = new KafkaTransactionManager(producerFactory());
        manager.setTransactionSynchronization(AbstractPlatformTransactionManager.SYNCHRONIZATION_ON_ACTUAL_TRANSACTION);
        return manager;
    }
}

/**
 * äº‹åŠ¡æ¶ˆæ¯æœåŠ¡
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class TransactionalMessageService {
    
    private final KafkaTemplate<String, Object> kafkaTemplate;
    private final UserService userService;
    private final OrderService orderService;
    
    /**
     * äº‹åŠ¡æ€§åˆ›å»ºç”¨æˆ·å’Œå‘é€æ¶ˆæ¯
     */
    @Transactional("kafkaTransactionManager")
    public void createUserWithEvent(CreateUserRequest request) {
        try {
            // 1. åˆ›å»ºç”¨æˆ·
            User user = userService.createUser(request);
            
            // 2. å‘é€ç”¨æˆ·åˆ›å»ºäº‹ä»¶
            UserCreatedEvent event = new UserCreatedEvent(user);
            String topicName = "dev.user-service.account.user-created.v1";
            
            kafkaTemplate.send(topicName, user.getId().toString(), event);
            
            // 3. æ›´æ–°ç”¨æˆ·ç»Ÿè®¡
            userService.updateUserCount();
            
            log.info("äº‹åŠ¡æ€§ç”¨æˆ·åˆ›å»ºå®Œæˆ: userId={}", user.getId());
            
        } catch (Exception e) {
            log.error("äº‹åŠ¡æ€§ç”¨æˆ·åˆ›å»ºå¤±è´¥", e);
            throw new RuntimeException("ç”¨æˆ·åˆ›å»ºäº‹åŠ¡å¤±è´¥", e);
        }
    }
    
    /**
     * åˆ†å¸ƒå¼äº‹åŠ¡å¤„ç†è®¢å•
     */
    @Transactional
    @KafkaTransactional
    public void processOrderTransaction(Long orderId) {
        try {
            // 1. æ›´æ–°è®¢å•çŠ¶æ€
            Order order = orderService.updateOrderStatus(orderId, OrderStatus.PROCESSING);
            
            // 2. æ‰£å‡åº“å­˜
            order.getItems().forEach(item -> 
                orderService.decreaseProductStock(item.getProductId(), item.getQuantity()));
            
            // 3. å‘é€è®¢å•å¤„ç†äº‹ä»¶
            OrderProcessingEvent event = new OrderProcessingEvent(order);
            kafkaTemplate.send("dev.order-service.order.order-processing.v1", 
                order.getOrderNo(), event);
            
            // 4. åˆ›å»ºæ”¯ä»˜è®°å½•
            orderService.createPaymentRecord(order);
            
            log.info("è®¢å•äº‹åŠ¡å¤„ç†å®Œæˆ: orderId={}", orderId);
            
        } catch (Exception e) {
            log.error("è®¢å•äº‹åŠ¡å¤„ç†å¤±è´¥: orderId={}", orderId, e);
            throw new RuntimeException("è®¢å•å¤„ç†äº‹åŠ¡å¤±è´¥", e);
        }
    }
    
    /**
     * ä½¿ç”¨Kafkaäº‹åŠ¡æ‰§è¡Œæ‰¹é‡æ“ä½œ
     */
    public void executeBatchOperationInTransaction(List<Long> userIds) {
        kafkaTemplate.executeInTransaction(operations -> {
            userIds.forEach(userId -> {
                try {
                    // è·å–ç”¨æˆ·ä¿¡æ¯
                    User user = userService.findById(userId);
                    
                    // å‘é€ç”¨æˆ·æ›´æ–°äº‹ä»¶
                    UserUpdatedEvent event = new UserUpdatedEvent(user);
                    operations.send("dev.user-service.account.user-updated.v1", 
                        userId.toString(), event);
                    
                    // æ›´æ–°ç”¨æˆ·æœ€åæ´»è·ƒæ—¶é—´
                    userService.updateLastActiveTime(userId);
                    
                } catch (Exception e) {
                    log.error("æ‰¹é‡æ“ä½œå¤„ç†ç”¨æˆ·å¤±è´¥: userId={}", userId, e);
                    throw new RuntimeException("æ‰¹é‡æ“ä½œå¤±è´¥", e);
                }
            });
            
            return null;
        });
        
        log.info("æ‰¹é‡äº‹åŠ¡æ“ä½œå®Œæˆ: userCount={}", userIds.size());
    }
}
// [AI-BLOCK-END]
```

## ğŸ“‹ ç›‘æ§å’Œè¿ç»´

### ç›‘æ§é…ç½®
```java
// [AI-BLOCK-START] - ç”Ÿæˆå·¥å…·: Claude 3.5 Sonnet
/**
 * Kafkaç›‘æ§æœåŠ¡
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class KafkaMonitoringService {
    
    private final MeterRegistry meterRegistry;
    private final KafkaAdmin kafkaAdmin;
    
    // ç›‘æ§æŒ‡æ ‡
    private final Gauge consumerLag = Gauge.builder("kafka.consumer.lag")
        .description("æ¶ˆè´¹è€…å»¶è¿Ÿ")
        .register(meterRegistry);
    
    private final Counter messageCount = Counter.builder("kafka.message.count")
        .description("æ¶ˆæ¯æ•°é‡")
        .register(meterRegistry);
    
    private final Timer messageProcessingTime = Timer.builder("kafka.message.processing.time")
        .description("æ¶ˆæ¯å¤„ç†æ—¶é—´")
        .register(meterRegistry);
    
    /**
     * ç›‘æ§æ¶ˆè´¹è€…å»¶è¿Ÿ
     */
    @Scheduled(fixedRate = 30000) // æ¯30ç§’æ‰§è¡Œä¸€æ¬¡
    public void monitorConsumerLag() {
        try {
            AdminClient adminClient = AdminClient.create(kafkaAdmin.getConfigurationProperties());
            
            // è·å–æ‰€æœ‰æ¶ˆè´¹è€…ç»„
            ListConsumerGroupsResult consumerGroups = adminClient.listConsumerGroups();
            consumerGroups.all().get().forEach(consumerGroup -> {
                try {
                    // è·å–æ¶ˆè´¹è€…ç»„è¯¦æƒ…
                    DescribeConsumerGroupsResult groupDescription = 
                        adminClient.describeConsumerGroups(Collections.singletonList(consumerGroup.groupId()));
                    
                    ConsumerGroupDescription description = groupDescription.all().get()
                        .get(consumerGroup.groupId());
                    
                    if (description.state() == ConsumerGroupState.STABLE) {
                        // è·å–æ¶ˆè´¹è€…å»¶è¿Ÿä¿¡æ¯
                        ListConsumerGroupOffsetsResult offsetsResult = 
                            adminClient.listConsumerGroupOffsets(consumerGroup.groupId());
                        
                        Map<TopicPartition, OffsetAndMetadata> offsets = offsetsResult.partitionsToOffsetAndMetadata().get();
                        
                        offsets.forEach((topicPartition, offsetMetadata) -> {
                            long currentOffset = offsetMetadata.offset();
                            
                            // è·å–ä¸»é¢˜åˆ†åŒºçš„æœ€æ–°åç§»é‡
                            long latestOffset = getLatestOffset(topicPartition);
                            long lag = latestOffset - currentOffset;
                            
                            // è®°å½•å»¶è¿ŸæŒ‡æ ‡
                            Gauge.builder("kafka.consumer.lag")
                                .tag("group", consumerGroup.groupId())
                                .tag("topic", topicPartition.topic())
                                .tag("partition", String.valueOf(topicPartition.partition()))
                                .register(meterRegistry, lag);
                            
                            if (lag > 1000) { // å»¶è¿Ÿè¶…è¿‡1000æ¡æ¶ˆæ¯æ—¶å‘Šè­¦
                                log.warn("æ¶ˆè´¹è€…å»¶è¿Ÿè¿‡é«˜: group={}, topic={}, partition={}, lag={}", 
                                    consumerGroup.groupId(), topicPartition.topic(), 
                                    topicPartition.partition(), lag);
                            }
                        });
                    }
                    
                } catch (Exception e) {
                    log.error("ç›‘æ§æ¶ˆè´¹è€…ç»„å¤±è´¥: group={}", consumerGroup.groupId(), e);
                }
            });
            
        } catch (Exception e) {
            log.error("ç›‘æ§æ¶ˆè´¹è€…å»¶è¿Ÿå¤±è´¥", e);
        }
    }
    
    /**
     * ç›‘æ§ä¸»é¢˜åˆ†åŒºä¿¡æ¯
     */
    @Scheduled(fixedRate = 60000) // æ¯1åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
    public void monitorTopicPartitions() {
        try {
            AdminClient adminClient = AdminClient.create(kafkaAdmin.getConfigurationProperties());
            
            // è·å–æ‰€æœ‰ä¸»é¢˜
            ListTopicsResult topicsResult = adminClient.listTopics();
            Set<String> topicNames = topicsResult.names().get();
            
            // è·å–ä¸»é¢˜æè¿°
            DescribeTopicsResult topicDescriptions = adminClient.describeTopics(topicNames);
            topicDescriptions.all().get().forEach((topicName, description) -> {
                
                int partitionCount = description.partitions().size();
                
                // è®°å½•åˆ†åŒºæ•°é‡æŒ‡æ ‡
                Gauge.builder("kafka.topic.partitions")
                    .tag("topic", topicName)
                    .register(meterRegistry, partitionCount);
                
                // æ£€æŸ¥å‰¯æœ¬çŠ¶æ€
                description.partitions().forEach(partition -> {
                    int replicationFactor = partition.replicas().size();
                    int inSyncReplicas = partition.isr().size();
                    
                    if (inSyncReplicas < replicationFactor) {
                        log.warn("ä¸»é¢˜åˆ†åŒºå‰¯æœ¬ä¸åŒæ­¥: topic={}, partition={}, replicas={}, isr={}", 
                            topicName, partition.partition(), replicationFactor, inSyncReplicas);
                    }
                });
                
                log.debug("ä¸»é¢˜ç›‘æ§: topic={}, partitions={}", topicName, partitionCount);
            });
            
        } catch (Exception e) {
            log.error("ç›‘æ§ä¸»é¢˜åˆ†åŒºå¤±è´¥", e);
        }
    }
    
    /**
     * å¥åº·æ£€æŸ¥
     */
    @EventListener
    public void handleKafkaHealthCheck(HealthCheckEvent event) {
        try {
            // æ£€æŸ¥Kafkaè¿æ¥çŠ¶æ€
            AdminClient adminClient = AdminClient.create(kafkaAdmin.getConfigurationProperties());
            DescribeClusterResult clusterResult = adminClient.describeCluster();
            
            int nodeCount = clusterResult.nodes().get().size();
            String clusterId = clusterResult.clusterId().get();
            
            log.info("Kafkaé›†ç¾¤å¥åº·æ£€æŸ¥: clusterId={}, nodes={}", clusterId, nodeCount);
            
            // è®°å½•é›†ç¾¤èŠ‚ç‚¹æ•°æŒ‡æ ‡
            Gauge.builder("kafka.cluster.nodes")
                .register(meterRegistry, nodeCount);
            
        } catch (Exception e) {
            log.error("Kafkaå¥åº·æ£€æŸ¥å¤±è´¥", e);
            
            // è®°å½•å¥åº·æ£€æŸ¥å¤±è´¥æŒ‡æ ‡
            Counter.builder("kafka.health.check.failed")
                .register(meterRegistry)
                .increment();
        }
    }
    
    /**
     * è®°å½•æ¶ˆæ¯å¤„ç†æŒ‡æ ‡
     */
    public void recordMessageProcessed(String topic, String consumerGroup, boolean success, long processingTimeMs) {
        String status = success ? "success" : "error";
        
        messageCount.increment(Tags.of(
            "topic", topic,
            "consumer_group", consumerGroup,
            "status", status
        ));
        
        messageProcessingTime.record(processingTimeMs, TimeUnit.MILLISECONDS, Tags.of(
            "topic", topic,
            "consumer_group", consumerGroup
        ));
    }
    
    private long getLatestOffset(TopicPartition topicPartition) {
        // å®ç°è·å–æœ€æ–°åç§»é‡çš„é€»è¾‘
        // è¿™é‡Œéœ€è¦ä½¿ç”¨KafkaConsumeræˆ–AdminClientæ¥è·å–
        return 0L;
    }
}

/**
 * Kafkaå¥åº·æ£€æŸ¥æŒ‡ç¤ºå™¨
 */
@Component
@RequiredArgsConstructor
public class KafkaHealthIndicator implements HealthIndicator {
    
    private final KafkaAdmin kafkaAdmin;
    
    @Override
    public Health health() {
        try {
            AdminClient adminClient = AdminClient.create(kafkaAdmin.getConfigurationProperties());
            
            // æ£€æŸ¥é›†ç¾¤è¿æ¥
            DescribeClusterResult clusterResult = adminClient.describeCluster();
            Collection<Node> nodes = clusterResult.nodes().get(5, TimeUnit.SECONDS);
            
            if (nodes.isEmpty()) {
                return Health.down()
                    .withDetail("error", "No Kafka nodes available")
                    .build();
            }
            
            return Health.up()
                .withDetail("cluster", clusterResult.clusterId().get())
                .withDetail("nodes", nodes.size())
                .withDetail("brokers", nodes.stream()
                    .map(node -> node.host() + ":" + node.port())
                    .collect(Collectors.toList()))
                .build();
                
        } catch (Exception e) {
            return Health.down()
                .withDetail("error", e.getMessage())
                .withException(e)
                .build();
        }
    }
}
// [AI-BLOCK-END]
```

## âœ… æ£€æŸ¥æ¸…å•

### ä¸»é¢˜è®¾è®¡æ£€æŸ¥
- [ ] ä¸»é¢˜å‘½åéµå¾ªæ ‡å‡†æ ¼å¼ï¼š{ç¯å¢ƒ}.{æœåŠ¡}.{åŸŸ}.{äº‹ä»¶}.{ç‰ˆæœ¬}
- [ ] åˆ†åŒºæ•°é‡æ ¹æ®é¢„æœŸååé‡åˆç†è®¾ç½®
- [ ] å‰¯æœ¬å› å­æ»¡è¶³é«˜å¯ç”¨è¦æ±‚
- [ ] æ¶ˆæ¯ä¿ç•™æœŸé™ç¬¦åˆä¸šåŠ¡éœ€æ±‚

### æ¶ˆæ¯æ ¼å¼æ£€æŸ¥
- [ ] æ¶ˆæ¯åŒ…å«å¿…è¦çš„å…ƒæ•°æ®å­—æ®µ
- [ ] æ¶ˆæ¯æ”¯æŒç‰ˆæœ¬ç®¡ç†å’Œå‘åå…¼å®¹
- [ ] æ¶ˆæ¯å¤§å°æ§åˆ¶åœ¨åˆç†èŒƒå›´å†…
- [ ] æ•æ„Ÿä¿¡æ¯å·²åŠ å¯†æˆ–è„±æ•

### ç”Ÿäº§è€…æ£€æŸ¥
- [ ] å¯ç”¨å¹‚ç­‰æ€§å’Œäº‹åŠ¡æ”¯æŒ
- [ ] é…ç½®åˆé€‚çš„é‡è¯•ç­–ç•¥
- [ ] å®ç°å‘é€å¤±è´¥å¤„ç†é€»è¾‘
- [ ] æ‰¹é‡å‘é€ä¼˜åŒ–é…ç½®

### æ¶ˆè´¹è€…æ£€æŸ¥
- [ ] å®ç°å¹‚ç­‰æ€§æ¶ˆè´¹é€»è¾‘
- [ ] é…ç½®åˆç†çš„å¹¶å‘æ¶ˆè´¹æ•°é‡
- [ ] æ‰‹åŠ¨æäº¤åç§»é‡
- [ ] å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶å®Œå–„

### ç›‘æ§æ£€æŸ¥
- [ ] ç›‘æ§æ¶ˆè´¹è€…å»¶è¿Ÿ
- [ ] ç›‘æ§æ¶ˆæ¯å¤„ç†æ—¶é—´
- [ ] ç›‘æ§é”™è¯¯ç‡å’Œé‡è¯•æ¬¡æ•°
- [ ] è®¾ç½®å…³é”®æŒ‡æ ‡å‘Šè­¦

---
*éµå¾ªä»¥ä¸ŠKafkaè§„èŒƒï¼Œç¡®ä¿æ¶ˆæ¯ç³»ç»Ÿçš„é«˜å¯é æ€§ã€é«˜æ€§èƒ½å’Œå¯ç»´æŠ¤æ€§*
description:
globs:
alwaysApply: false
---
